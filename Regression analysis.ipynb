{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rushi\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prettytable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e177fc323470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mprettytable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVR\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'prettytable'"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import sklearn\n",
    "import matplotlib.pyplot as pypl\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import KFold\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "# function that calculates adjusted R2 value as defined in \n",
    "# https://sourceforge.net/p/scikit-learn/mailman/scikit-learn-general/thread/516EC479.6080704@gmail.com/\n",
    "def adj_r2_score(model,y,yhat):\n",
    "\tadj = 1 - float(len(y)-1)/(len(y)-len(model.coef_)-1)*(1 - metrics.r2_score(y,yhat))\n",
    "\treturn adj\n",
    "\n",
    "# ===============================================================\n",
    "# ================ HOLD.OUT (TEST/TRAIN) METHOD =================\n",
    "# ===============================================================\n",
    "\n",
    "# reading training data (year 2012 to 2014) and converting to data frame\n",
    "train = pandas.read_csv(\"training.csv\",encoding='utf-8')\n",
    "train_df = pandas.DataFrame(train)\n",
    "train_df.dropna(inplace = True)\n",
    "\n",
    "# setting up the data for prediction\n",
    "X = train_df[[\"teaching\",\"international\",\"research\",\"citations\",\"income\"]]\n",
    "y = train_df[[\"total_score\"]]\n",
    "\n",
    "r2_mul = []\n",
    "adj_r2_mul = []\n",
    "mse_mul = []\n",
    "\n",
    "r2_svr = []\n",
    "adj_r2_svr = []\n",
    "mse_svr = []\n",
    "\n",
    "for i in range(100):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4)\n",
    "\n",
    "\t# initiating the regression algorithms\n",
    "\n",
    "\tlreg = linear_model.LinearRegression()\t# multiple linear regression model\n",
    "\tlreg.fit(X_train, y_train)\n",
    "\ty_pred = lreg.predict(X_test)\n",
    "\t# print y_pred\n",
    "\tr2 = r2_score(y_test, y_pred)\n",
    "\tadj_r2 = adj_r2_score(lreg, y_test, y_pred)\n",
    "\tmse = mean_squared_error(y_test, y_pred)\n",
    "\tr2_mul.append(r2)\n",
    "\tadj_r2_mul.append(adj_r2)\n",
    "\tmse_mul.append(mse)\n",
    "\n",
    "\tsvr = SVR(kernel = 'linear')\t# support vector regression (Linear kernel)\n",
    "\tsvr.fit(X_train, numpy.ravel(y_train))\n",
    "\ty_pred = svr.predict(X_test)\n",
    "\t# print y_pred\n",
    "\tr2 = r2_score(y_test, y_pred)\n",
    "\tadj_r2 = adj_r2_score(svr, y_test, y_pred)\n",
    "\tmse = mean_squared_error(y_test, y_pred)\n",
    "\tr2_svr.append(r2)\n",
    "\tadj_r2_svr.append(adj_r2)\n",
    "\tmse_svr.append(mse)\n",
    "\n",
    "t = PrettyTable(['model','r2','adj.r2','mse'])\n",
    "t.add_row(['Multiple Regression', numpy.average(r2_mul), numpy.average(adj_r2_mul), numpy.average(mse_mul)])\n",
    "t.add_row(['Support Vector Regression', numpy.average(r2_svr), numpy.average(adj_r2_svr), numpy.average(mse_svr)])\n",
    "\n",
    "print (\"===============================\")\n",
    "print (\" Hold.out (test-train) method \")\n",
    "print (\"===============================\")\n",
    "print (t) \n",
    "\n",
    "# ===============================================================\n",
    "# ==================== CROSS VALIDATION =========================\n",
    "# ===============================================================\n",
    "\n",
    "data = pandas.read_csv(\"training.csv\",encoding='utf-8')\n",
    "df = pandas.DataFrame(data)\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "t1 = PrettyTable(['Multiple Regression','Fold','r2','adj.r2','mse'])\n",
    "t2 = PrettyTable(['Support Vector Regression','Fold','r2','adj.r2','mse'])\n",
    "\n",
    "X = df[[\"teaching\",\"international\",\"research\",\"citations\",\"income\"]]\n",
    "y = df[[\"total_score\"]]\n",
    "\n",
    "lreg = linear_model.LinearRegression()\t# multiple linear regression model\n",
    "svr = SVR(kernel = 'linear')\t# support vector regression (Linear kernel)\n",
    "\n",
    "kf = KFold(len(X), n_folds = 10)\n",
    "count = 0\n",
    "\n",
    "r2_mul = []\n",
    "adj_r2_mul = []\n",
    "mse_mul = []\n",
    "\n",
    "r2_svr = []\n",
    "adj_r2_svr = []\n",
    "mse_svr = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "\tX_train, X_test = X[train_index[0] : train_index[-1]], X[test_index[0] : test_index[-1]]\n",
    "\ty_train, y_test = y[train_index[0] : train_index[-1]], y[test_index[0] : test_index[-1]]\n",
    "\n",
    "\tcount += 1\n",
    "\n",
    "\tlreg.fit(X_train, y_train)\n",
    "\ty_pred = lreg.predict(X_test)\n",
    "\t# print y_pred\n",
    "\tr2 = r2_score(y_test, y_pred)\n",
    "\tadj_r2 = adj_r2_score(lreg, y_test, y_pred)\n",
    "\tmse = mean_squared_error(y_test, y_pred)\n",
    "\tr2_mul.append(r2)\n",
    "\tadj_r2_mul.append(adj_r2)\n",
    "\tmse_mul.append(mse)\n",
    "\tt1.add_row([' ', count, r2, adj_r2, mse])\n",
    "\n",
    "\tsvr = SVR(kernel = 'linear')\t# support vector regression (Linear kernel)\n",
    "\tsvr.fit(X_train, numpy.ravel(y_train))\n",
    "\ty_pred = svr.predict(X_test)\n",
    "\t# print y_pred\n",
    "\tr2 = r2_score(y_test, y_pred)\n",
    "\tadj_r2_mul.append(adj_r2)\n",
    "\tmse = mean_squared_error(y_test, y_pred)\n",
    "\tr2_svr.append(r2)\n",
    "\tadj_r2_svr.append(adj_r2)\n",
    "\tmse_svr.append(mse)\n",
    "\tt2.add_row([' ', count, r2, adj_r2, mse])\n",
    "\n",
    "t1.add_row(['average', \"\", numpy.average(r2_mul), numpy.average(adj_r2_mul), numpy.average(mse_mul)])\n",
    "t2.add_row(['average', \"\", numpy.average(r2_svr), numpy.average(adj_r2_svr), numpy.average(mse_svr)])\n",
    "\n",
    "print (\"===============================\")\n",
    "print (\" cross-validation method \")\n",
    "print (\"===============================\")\n",
    "print (t1)\n",
    "print (t2)\n",
    "\n",
    "# ===============================================================\n",
    "# ======== Testing multiple regression model on test set ========\n",
    "# ===============================================================\n",
    "\n",
    "# reading training data (year 2012 to 2014) and converting to data frame\n",
    "train = pandas.read_csv(\"training.csv\",encoding='utf-8')\n",
    "train_df = pandas.DataFrame(train)\n",
    "train_df.dropna(inplace = True)\n",
    "\n",
    "# reading test data (year 2015 and 2016 data) and converting to data frame\n",
    "test = pandas.read_csv(\"testing.csv\",encoding='utf-8')\n",
    "test_df = pandas.DataFrame(test)\n",
    "test_df.dropna(inplace = True)\n",
    "\n",
    "# setting up the data for prediction\n",
    "X_train = train_df[[\"teaching\",\"international\",\"research\",\"citations\",\"income\"]]\n",
    "y_train = train_df[[\"total_score\"]]\n",
    "X_test = test_df[[\"teaching\",\"international\",\"research\",\"citations\",\"income\"]]\n",
    "y_test = test_df[[\"total_score\"]]\n",
    "\n",
    "lreg = linear_model.LinearRegression()\t# multiple linear regression model\n",
    "lreg.fit(X_train, y_train)\n",
    "y_pred = lreg.predict(X_test)\n",
    "# print y_pred\t# remove the comment to print prediction results\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "adj_r2 = adj_r2_score(lreg, y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "t = PrettyTable(['Multiple regression (on test data)','r2','adj.r2','mse'])\n",
    "t.add_row(['', r2, adj_r2, mse])\n",
    "\n",
    "print (\"===============================\")\n",
    "print (\" Testing results \")\n",
    "print (\"===============================\")\n",
    "print (t)\n",
    "\n",
    "# ===============================================================\n",
    "# ======================== Graph plot ===========================\n",
    "# ===============================================================\n",
    "\n",
    "actual_scores = y_test\n",
    "actual_scores = numpy.ravel(actual_scores)\n",
    "actual_scores_2015 = actual_scores[0:50]\t# get actual scores of 50 universities for test set for year 2015\n",
    "actual_scores_2016 = actual_scores[188:(188+50)]\t# get actual scores for test set for year 2016\n",
    "\n",
    "test_scores = y_pred\n",
    "test_scores = numpy.ravel(test_scores)\n",
    "test_scores_2015 = test_scores[0:50]\t# get predicted scores of 50 universities for year 2015\n",
    "test_scores_2016 = test_scores[188:(188+50)]\t# get predicted scores for year 2016\n",
    "\n",
    "univ_names = test_df[[\"university_name\"]]\n",
    "univ_names = numpy.ravel(univ_names)\n",
    "for i in range(len(univ_names)):\n",
    "\ttemp = univ_names[i]\n",
    "\tuniv_names[i] = temp[0:20]\n",
    "univ_names_2015 = univ_names[0:50]\n",
    "univ_names_2016 = univ_names[188:(188+50)]\n",
    "\n",
    "# plotting 2015 university actual scores (oublished) vs. predicted scores\n",
    "x_2015 = numpy.array(range(len(univ_names_2015)))\n",
    "y_actual_2015 = actual_scores_2015\n",
    "y_test_2015 = test_scores_2015\n",
    "my_xticks_2015 = univ_names_2015\n",
    "\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.2)\n",
    "plt.xticks(x_2015, my_xticks_2015, rotation = 'vertical')\n",
    "plt.tick_params(axis='x', which='major', labelsize=8)\n",
    "plt.scatter(x_2015, y_actual_2015, marker = 'o', c = \"skyblue\", alpha = 0.3, s = 30, label = '2015 actual scores')\n",
    "plt.plot(x_2015, y_test_2015, linestyle = '--', c = \"red\", alpha = 1, lw = 1.5, label = '2015 predicted scores')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('university')\n",
    "plt.ylabel('overall_score')\n",
    "plt.title('2015 World University Rankings (Top 50)')\n",
    "plt.tight_layout\n",
    "plt.show()\n",
    "\n",
    "# plotting 2015 university actual scores (oublished) vs. predicted scores\n",
    "x_2016 = numpy.array(range(len(univ_names_2016)))\n",
    "y_actual_2016 = actual_scores_2016\n",
    "y_test_2016 = test_scores_2016\n",
    "my_xticks_2016 = univ_names_2016\n",
    "\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.2)\n",
    "plt.xticks(x_2016, my_xticks_2016, rotation = 'vertical')\n",
    "plt.tick_params(axis='x', which='major', labelsize=8)\n",
    "plt.xticks(x_2016, my_xticks_2016, rotation = 'vertical')\n",
    "plt.scatter(x_2016, y_actual_2016, marker = '^', c = \"lightgrey\", alpha = 0.3, s = 30, label = '2016 actual scores')\n",
    "plt.plot(x_2016, y_test_2016, linestyle = '--', c = \"deeppink\", alpha = 1, lw = 1.5, label = '2016 predicted scores')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('university')\n",
    "plt.ylabel('overall_score')\n",
    "plt.title('2016 World University Rankings (Top 50)')\n",
    "plt.show()\n",
    "\n",
    "# plotting top 50 universities of 2015 based on calculated scores\n",
    "x_2015 = numpy.array(range(len(univ_names_2015)))\n",
    "y_actual_2015 = actual_scores_2015\n",
    "y_test_2015 = test_scores_2015\n",
    "my_xticks_2015 = univ_names_2015\n",
    "\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.2)\n",
    "plt.xticks(x_2015, my_xticks_2015, rotation = 'vertical')\n",
    "plt.tick_params(axis='x', which='major', labelsize=8)\n",
    "plt.bar(x_2015, y_actual_2015, alpha = 0.3)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('university')\n",
    "plt.ylabel('overall_score')\n",
    "plt.title('2015 Top 50 Universities (based on predicted overall score)')\n",
    "plt.tight_layout\n",
    "plt.show()\n",
    "\n",
    "# plotting top 50 universities of 2016 based on calculated scores\n",
    "x_2016 = numpy.array(range(len(univ_names_2016)))\n",
    "y_actual_2016 = actual_scores_2016\n",
    "y_test_2016 = test_scores_2016\n",
    "my_xticks_2016 = univ_names_2016\n",
    "\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.2)\n",
    "plt.xticks(x_2016, my_xticks_2016, rotation = 'vertical')\n",
    "plt.tick_params(axis='x', which='major', labelsize=8)\n",
    "plt.bar(x_2016, y_actual_2016, alpha = 0.3)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('university')\n",
    "plt.ylabel('overall_score')\n",
    "plt.title('2016 Top 50 Universities (based on predicted overall score)')\n",
    "plt.tight_layout\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "actual_scores = y_test\n",
    "actual_scores = numpy.ravel(actual_scores)\n",
    "actual_scores_2015 = actual_scores[0:188]\t# get actual scores for test set for year 2015\n",
    "actual_scores_2016 = actual_scores[188:len(actual_scores)]\t# get actual scores for test set for year 2016\n",
    "test_scores = y_pred\n",
    "test_scores = numpy.ravel(test_scores)\n",
    "test_scores_2015 = test_scores[0:188]\t# get predicted scores for year 2015\n",
    "test_scores_2016 = test_scores[188:len(test_scores)]\t# get predicted scores for year 2016\n",
    "univ_names = test_df[[\"university_name\"]]\n",
    "univ_names = numpy.ravel(univ_names)\n",
    "univ_names_2015 = univ_names[0:188]\n",
    "univ_names_2016 = univ_names[188:len(univ_names)]\n",
    "x_2015 = numpy.array(range(len(univ_names_2015)))\n",
    "y_actual_2015 = actual_scores_2015\n",
    "y_test_2015 = test_scores_2015\n",
    "plt.scatter(x_2015, y_actual_2015, marker = 'o', c = \"skyblue\", alpha = '0.2', s = 30, label = '2015 actual scores')\n",
    "plt.plot(x_2015, y_test_2015, linestyle = '--', c = \"red\", alpha = 1, lw = 1.5, label = '2015 predicted scores')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('university')\n",
    "plt.ylabel('overall_score')\n",
    "plt.title('2015 world university rankings')\n",
    "plt.show()\n",
    "x_2016 = numpy.array(range(len(univ_names_2016)))\n",
    "y_actual_2016 = actual_scores_2016\n",
    "y_test_2016 = test_scores_2016\n",
    "plt.scatter(x_2016, y_actual_2016, marker = '^', c = \"lightgrey\", alpha = '0.2', s = 30, label = '2016 actual scores')\n",
    "plt.plot(x_2016, y_test_2016, linestyle = '--', c = \"deeppink\", alpha = 1, lw = 1.5, label = '2016 predicted scores')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('university')\n",
    "plt.ylabel('overall_score')\n",
    "plt.title('2016 world university rankings')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "x_2016 = numpy.array(range(len(univ_names_2016)))\n",
    "y_actual_2016 = actual_scores_2016\n",
    "y_test_2016 = test_scores_2016\n",
    "plt.scatter(x_2016, y_actual_2016, marker = 'o', c = \"skyblue\", alpha = '0.2', s = 30, label = '2016 actual scores')\n",
    "plt.plot(x_2016, y_test_2016, linestyle = '--', c = \"red\", alpha = 1, lw = 1.5, label = '2016 predicted scores')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('university')\n",
    "plt.ylabel('overall_score')\n",
    "plt.title('2015 world university rankings')\n",
    "x_2016 = numpy.array(range(len(univ_names_2016)))\n",
    "y_actual_2016 = actual_scores_2016\n",
    "y_test_2016 = test_scores_2016\n",
    "plt.plot(x_2016, y_actual_2016, c = \"green\")\n",
    "plt.plot(x_2016, y_test_2016, linestyle = \"--\", c = \"red\")\n",
    "x_2016 = numpy.array(range(len(actual_scores_2016)))\n",
    "y_2016 = actual_scores_2016\n",
    "my_xticks_2016 = univ_names_2016\n",
    "#plt.xticks(x_2015, my_xticks_2015, rotation = 'vertical')\n",
    "plt.plot(x_2015, y_2015, c = \"red\")\n",
    "plt.plot(x_2016, y_2016, c = \"green\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
